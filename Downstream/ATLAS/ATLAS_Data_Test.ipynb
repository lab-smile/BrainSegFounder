{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f23a75-f241-4c7d-8087-795624bbc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import PathLike\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import monai.transforms\n",
    "from monai import data, transforms\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3c58e9-df7a-4a69-856b-da92064e11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True, make_even=True):\n",
    "        if num_replicas is None:\n",
    "            if not torch.distributed.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            num_replicas = torch.distributed.get_world_size()\n",
    "        if rank is None:\n",
    "            if not torch.distributed.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            rank = torch.distributed.get_rank()\n",
    "        self.shuffle = shuffle\n",
    "        self.make_even = make_even\n",
    "        self.dataset = dataset\n",
    "        self.num_replicas = num_replicas\n",
    "        self.rank = rank\n",
    "        self.epoch = 0\n",
    "        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n",
    "        self.total_size = self.num_samples * self.num_replicas\n",
    "        indices = list(range(len(self.dataset)))\n",
    "        self.valid_length = len(indices[self.rank: self.total_size: self.num_replicas])\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            g = torch.Generator()\n",
    "            g.manual_seed(self.epoch)\n",
    "            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n",
    "        else:\n",
    "            indices = list(range(len(self.dataset)))\n",
    "        if self.make_even:\n",
    "            if len(indices) < self.total_size:\n",
    "                if self.total_size - len(indices) < len(indices):\n",
    "                    indices += indices[: (self.total_size - len(indices))]\n",
    "                else:\n",
    "                    extra_ids = np.random.randint(low=0, high=len(indices), size=self.total_size - len(indices))\n",
    "                    indices += [indices[ids] for ids in extra_ids]\n",
    "            assert len(indices) == self.total_size\n",
    "        indices = indices[self.rank: self.total_size: self.num_replicas]\n",
    "        self.num_samples = len(indices)\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "\n",
    "def split_atlas_data(data_directory: str, train_fraction: float = 0.8, seed: int = None) -> tuple:\n",
    "    \"\"\"Locates and splits ATLAS 2.0 Data into training and validation lists.\n",
    "\n",
    "    :param data_directory: absolute path to the source file. For ATLAS should be /red/ruogu.fang/atlas/decrypt/ATLAS_2\n",
    "    :param train_fraction: percentage of the data split into training data\n",
    "    :param seed: random seed for splitting data. If None, data will be split differently each time.\n",
    "    :returns: tuple of training data, validation data. Each item in the tuple is a list of dictionaries pointing to the\n",
    "        with keys 'image' and 'label', similar to how the BraTS data is treated\n",
    "    \"\"\"\n",
    "\n",
    "    data_directory = Path(data_directory)\n",
    "    training_dir = data_directory / 'Training'\n",
    "    subdirectory = 'ses-1/anat/'\n",
    "\n",
    "    training = []\n",
    "    for record_id in training_dir.iterdir():\n",
    "        for subject_id in record_id.iterdir():\n",
    "            if str(subject_id).endswith('.json'):\n",
    "                continue \n",
    "            scan_path = training_dir / record_id / subject_id / subdirectory\n",
    "            label = f'{scan_path}/{os.path.basename(subject_id)}_ses-1_space-MNI152NLin2009aSym_label-L_desc-T1lesion_mask.nii.gz'\n",
    "            image = f'{scan_path}/{os.path.basename(subject_id)}_ses-1_space-MNI152NLin2009aSym_T1w.nii.gz'\n",
    "\n",
    "            training.append(\n",
    "                {\n",
    "                    'label': label,\n",
    "                    'image': image\n",
    "                }\n",
    "            )\n",
    "            \n",
    "    train, val = train_test_split(training, train_size=train_fraction, random_state=seed)\n",
    "    return train, val\n",
    "\n",
    "\n",
    "def get_loaders(data_directory: str,\n",
    "                batch_size: int,\n",
    "                roi: tuple = (96, 128, 128),\n",
    "                train_fraction: float = 0.8,\n",
    "                seed: int = None,\n",
    "                n_workers: int = 1,\n",
    "                distributed=False):\n",
    "    \"\"\"Create training and validation dataloaders for the ALTAS Dataset\n",
    "\n",
    "    :param data_directory: absolute path to the high level directory for data. For ATLAS,\n",
    "        should be /red/ruogu.fang/atlas/decrypt/ATLAS_2.\n",
    "    :param batch_size: Batch size for training stage (validation size is 1)\n",
    "    :param roi: Region of interest, images will be cropped to this size (x, y, z)\n",
    "    :param train_fraction: percentage of the data split into training data\n",
    "    :param seed: random seed for splitting data. If None, data will be split differently each time.\n",
    "    :param n_workers: number of workers for each dataloader. Set to max number of CPU cores available for best results\n",
    "    :param distributed: Whether or not to use multi-GPU sampler. Set to True if using multi-GPU training.\n",
    "    \"\"\"\n",
    "\n",
    "    train_files, validation_files = split_atlas_data(data_directory, train_fraction, seed)\n",
    "    train_transforms, validation_transforms = get_transforms(roi)\n",
    "\n",
    "    train_dataset = data.Dataset(data=train_files, transform=train_transforms)\n",
    "    train_sampler = Sampler(train_dataset) if distributed else None\n",
    "    train_loader = data.DataLoader(train_dataset,\n",
    "                                   num_workers=n_workers,\n",
    "                                   shuffle=(not distributed),\n",
    "                                   batch_size=batch_size,\n",
    "                                   pin_memory=True,\n",
    "                                   sampler=train_sampler)\n",
    "\n",
    "    validation_dataset = data.Dataset(data=validation_files, transform=validation_transforms)\n",
    "    validation_sampler = Sampler(validation_dataset) if distributed else None\n",
    "    validation_loader = data.DataLoader(validation_dataset,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=n_workers,\n",
    "                                        sampler=validation_sampler,\n",
    "                                        pin_memory=True)\n",
    "\n",
    "    return train_loader, validation_loader\n",
    "\n",
    "\n",
    "def get_transforms(roi: tuple) -> tuple:\n",
    "    \"\"\"Creates two transforms for the data, one for training and validation. The training transform consists of\n",
    "        random image augmentation steps, controlled by MONAI's global seed. If you want to change what augmentation\n",
    "        is performed during this stage of pretraining, this is where you change it. For more information on the\n",
    "        transforms used, see the README for this downstream Task\n",
    "\n",
    "    :param roi: tuple of 3 integers, the desired size of the output images in x, y, z coordinates.\n",
    "    :returns: tuple of monai.transform.Compose objects (training, validation).\n",
    "    \"\"\"\n",
    "    all_keys = ['image', 'label']\n",
    "\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=all_keys),\n",
    "            transforms.RepeatChanneld(keys='image', repeats=1),  # Repeat channel to match 2 channel input\n",
    "            transforms.NormalizeIntensityd(keys='image', nonzero=True, channel_wise=True),\n",
    "            transforms.RandScaleIntensityd(keys='image', factors=0.1, prob=1.0),\n",
    "            transforms.RandShiftIntensityd(keys='image', offsets=0.1, prob=1.0),\n",
    "            transforms.Resized(keys=all_keys, spatial_size=roi),\n",
    "            transforms.ToTensord(keys=all_keys)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=all_keys),\n",
    "            transforms.RepeatChanneld(keys='image', repeats=1),\n",
    "            transforms.Resized(keys=all_keys, spatial_size=roi),\n",
    "            transforms.ToTensord(keys=all_keys)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return train_transform, val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca4b8ff-fa32-4b19-a32c-564ae686ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, va = get_loaders('/red/ruogu.fang/atlas/decrypt/ATLAS_2', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e5d7f-1792-430c-b98f-563c79293854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea7881-0f41-466b-81ba-e8eb94ffa5da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GatorBrain",
   "language": "python",
   "name": "gatorbrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
