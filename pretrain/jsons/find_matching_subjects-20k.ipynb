{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79f6419-a262-4d73-8b07-d100a61db72f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be63af4-28bb-4a1a-9a6e-6e7146fbad5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## CONSTANTS\n",
    "DATA_DIR = '/red/ruogu.fang/share/UKB/data/Brain'\n",
    "T1_FILE_PATH = '20252_T1_NIFTI'\n",
    "T2_FILE_PATH = '20253_T2_NIFTI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "744496af-1727-414c-b9b0-0178a89fc388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = Path(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7202dc8-f381-4505-afaf-be5b41b97d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_id_from_file(filename: str) -> str: \n",
    "    '''Split patient ID from a filename. Note that this function\n",
    "    does not take into account the visit number, nor number of s\n",
    "    '''\n",
    "    return filename.split('_')[0]  # First section is id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac13908b-e064-4b21-98db-cf78f311c6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ids_to_json(ids: set, outdir: os.PathLike, \n",
    "                name='GatorBrain_matched_subjects.json') -> None:\n",
    "    outdir = Path(outdir)\n",
    "    with open(outdir / name, 'w') as json_file:\n",
    "        json.dump(list(ids), json_file)\n",
    "    print(f'Saved matching subject IDs to {outdir / name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e46d92-f54f-48ff-9992-0b5d2212c1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1_path = data_path / T1_FILE_PATH\n",
    "t2_path = data_path / T2_FILE_PATH / 'T2_unzip'\n",
    "\n",
    "# T1 data is stored in two different areas\n",
    "new_t1_path = t1_path / 'T1_new_unzip'\n",
    "old_t1_path = t1_path / 'T1_unzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e57c9964-9b9c-4a9d-a910-989438d5e42c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_t1_ids = [split_id_from_file(filename.name) \n",
    "              for filename in old_t1_path.iterdir()]\n",
    "\n",
    "new_t1_ids = [split_id_from_file(filename.name) \n",
    "              for filename in new_t1_path.iterdir()]\n",
    "\n",
    "t1_ids = set(old_t1_ids).union(set(new_t1_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ca45828-870e-4656-a7ea-3342d348b594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2_ids = [split_id_from_file(filename.name)\n",
    "          for filename in t2_path.iterdir()]\n",
    "t2_ids = set(t2_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64a61d3-6419-4183-a932-4a4e2c034af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_ids = t1_ids.intersection(t2_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28d4436e-dce1-4c12-89bb-5df91f94d26f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 44172 unique T1 subjects\n",
      "Total of 43369 unique T2 subjects\n",
      "Total of 43367 matching subjects found\n",
      "Saved matching subject IDs to GatorBrain_matched_subjects.json\n"
     ]
    }
   ],
   "source": [
    "print(f'Total of {len(t1_ids)} unique T1 subjects')\n",
    "print(f'Total of {len(t2_ids)} unique T2 subjects')\n",
    "print(f'Total of {len(matching_ids)} matching subjects found')\n",
    "ids_to_json(matching_ids, outdir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3ff2fa3-97bf-4844-b823-205a72094edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datalist = list(matching_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509e532-d9e8-4b7a-b9b9-333da43ecaf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Format 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa6d8eb-12be-4741-aeba-fe0d82f65f13",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### train data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f172cf37-149c-46b2-b1ed-1c917790dac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unused T1 images = 1289\n",
      "number of unused T2 images = 1845\n",
      "number of usable subjects with both T1 + T2 images = 39495\n"
     ]
    }
   ],
   "source": [
    "train_datalist=[]\n",
    "unused_T1 = []\n",
    "unused_T2 = []\n",
    "for subject in datalist[:-2000]:\n",
    "    old_image_t1 = str(old_t1_path) + '/' + subject + \"_20252_2_0/T1_brain_to_MNI.nii.gz\"\n",
    "    new_image_t1 = str(new_t1_path) + '/' + subject + \"_20252_2_0/T1/T1_brain_to_MNI.nii.gz\"\n",
    "    image_t2 = str(t2_path) + '/' + subject + \"_20253_2_0/T2_FLAIR/T2_FLAIR_brain_to_MNI.nii.gz\"\n",
    "\n",
    "    usable=True\n",
    "    \n",
    "    if os.path.exists(old_image_t1):\n",
    "        image_t1 = old_image_t1 \n",
    "    elif os.path.exists(new_image_t1):\n",
    "        image_t1 = new_image_t1 \n",
    "    else:\n",
    "        #print(f\"subject:{subject} T1 not found\")\n",
    "        usable=False\n",
    "        unused_T1.append(subject)\n",
    "        \n",
    "    if os.path.exists(image_t2):\n",
    "        pass\n",
    "    else:\n",
    "        #print(f\"subject:{subject} T2 not found\")\n",
    "        usable=False\n",
    "        unused_T2.append(subject)\n",
    "    \n",
    "    if usable:\n",
    "        train_datalist.append({\"image_T1\": image_t1, \"image_T2\": image_t2})\n",
    "\n",
    "print(f\"number of unused T1 images = {len(unused_T1)}\")\n",
    "print(f\"number of unused T2 images = {len(unused_T2)}\")\n",
    "print(f\"number of usable subjects with both T1 + T2 images = {len(train_datalist)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966961f-225c-4462-905d-5cfbedba0698",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### validation data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b06fe55-67d7-46d2-87d0-0ac3b2e2d350",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unused T1 images = 67\n",
      "number of unused T2 images = 93\n",
      "number of usable subjects with both T1 + T2 images = 1905\n"
     ]
    }
   ],
   "source": [
    "val_datalist=[]\n",
    "unused_T1 = []\n",
    "unused_T2 = []\n",
    "for subject in datalist[-2000:]:\n",
    "    old_image_t1 = str(old_t1_path) + '/' + subject + \"_20252_2_0/T1_brain_to_MNI.nii.gz\"\n",
    "    new_image_t1 = str(new_t1_path) + '/' + subject + \"_20252_2_0/T1/T1_brain_to_MNI.nii.gz\"\n",
    "    image_t2 = str(t2_path) + '/' + subject + \"_20253_2_0/T2_FLAIR/T2_FLAIR_brain_to_MNI.nii.gz\"\n",
    "\n",
    "    usable=True\n",
    "    \n",
    "    if os.path.exists(old_image_t1):\n",
    "        image_t1 = old_image_t1 \n",
    "    elif os.path.exists(new_image_t1):\n",
    "        image_t1 = new_image_t1 \n",
    "    else:\n",
    "        #print(f\"subject:{subject} T1 not found\")\n",
    "        usable=False\n",
    "        unused_T1.append(subject)\n",
    "        \n",
    "    if os.path.exists(image_t2):\n",
    "        pass\n",
    "    else:\n",
    "        #print(f\"subject:{subject} T2 not found\")\n",
    "        usable=False\n",
    "        unused_T2.append(subject)\n",
    "    \n",
    "    if usable:\n",
    "        val_datalist.append({\"image_T1\": image_t1, \"image_T2\": image_t2})\n",
    "\n",
    "print(f\"number of unused T1 images = {len(unused_T1)}\")\n",
    "print(f\"number of unused T2 images = {len(unused_T2)}\")\n",
    "print(f\"number of usable subjects with both T1 + T2 images = {len(val_datalist)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0171a-7b6f-45fa-9d49-ece62ffe28d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### select data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11732644-7e80-4e83-ab72-fb6bf41c7045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### reformat the data list\n",
    "train_datalist_reformat = [{\"image\":[entry[\"image_T1\"],entry[\"image_T2\"]]}  for entry in train_datalist ]\n",
    "\n",
    "val_datalist_reformat = [{\"image\":[entry[\"image_T1\"],entry[\"image_T2\"]]} for entry in val_datalist ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83a8b8ca-fb33-46bd-89b7-99b37c0006df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_datalist_10k = random.sample(train_datalist_reformat, 10000)\n",
    "train_datalist_20k = random.sample(train_datalist_reformat, 20000)\n",
    "\n",
    "val_datalist_10k   = random.sample(val_datalist_reformat, 1000) \n",
    "val_datalist_20k   = random.sample(val_datalist_reformat, 1000) \n",
    "\n",
    "data_dump_10k = {\"training\":train_datalist_10k,\n",
    "        \"validation\":val_datalist_10k}\n",
    "\n",
    "data_dump_20k = {\"training\":train_datalist_20k,\n",
    "        \"validation\":val_datalist_20k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c1345b8-375a-446b-856d-f575b6976c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_dump(data_dump, filename):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data_dump, json_file)\n",
    "\n",
    "data_dump(data_dump_10k, \"GBR_T1T2_matched_10k.json\")\n",
    "data_dump(data_dump_20k, \"GBR_T1T2_matched_20k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "047fa6b6-3a6b-4bd1-bd40-00ad547faea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb6cfd-8da7-4e0c-8f26-d8f3dbc2a786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ce74704-8e10-4f3d-8767-eea71be2c484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_T1T2mixed_datalist(args, input_file: str):\n",
    "    with open(input_file, 'r') as f:\n",
    "        fold = json.load(f)\n",
    "    print(fold.keys())\n",
    "    training_images = fold['training'] # Should be list\n",
    "    validation_images = fold['validation']\n",
    "    t1_path = args['t1_path'] #'/red/ruogu.fang/UKB/data/Brain/20252_T1_NIFTI/T1_unzip'\n",
    "    t2_path = args['t2_path'] #'/red/ruogu.fang/UKB/data/Brain/20253_T2_NIFTI/T2_unzip'\n",
    "    training = {}\n",
    "    for i, image in enumerate(training_images):\n",
    "        image_t1 = t1_path + '/' + image + \"_20252_2_0/T1_brain_to_MNI.nii.gz\"\n",
    "        image_t2 = t2_path + '/' + image + \"_20253_2_0/T2_FLAIR/T2_FLAIR_brain_to_MNI.nii.gz\"\n",
    "        training[i] = {\"T1_image\": image_t1,\n",
    "                       \"T2_image\": image_t2}\n",
    "\n",
    "    validation = {}\n",
    "    for i, image in enumerate(validation_images):\n",
    "        image_t1 = t1_path + '/' + image + \"_20252_2_0/T1_brain_to_MNI.nii.gz\"\n",
    "        image_t2 = t2_path + '/' + image + \"_20253_2_0/T2_FLAIR/T2_FLAIR_brain_to_MNI.nii.gz\"\n",
    "        validation[i] = {\"T1_image\": image_t1,\n",
    "                         \"T2_image\": image_t2}\n",
    "    return {'training': training,\n",
    "            'validation': validation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02f5d808-584b-409e-8206-98a545b9d3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['training', 'validation'])\n"
     ]
    }
   ],
   "source": [
    "args={\n",
    "    't1_path':'/red/ruogu.fang/UKB/data/Brain/20252_T1_NIFTI/T1_unzip',\n",
    "    't2_path':'/red/ruogu.fang/UKB/data/Brain/20253_T2_NIFTI/T2_unzip'\n",
    "    }\n",
    "input_file=\"GBR_T1_T2_matched.json\"\n",
    "data = load_T1T2mixed_datalist(args, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ad5494c-6a9a-49c8-bc7e-07bb78761880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e20688-bde0-402a-96b9-3511c15f808a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a5496-2098-4d85-8605-7cb272aec540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dce253e4-ca03-4117-8c6c-0ddb096d7d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_T1T2matched_datalist(args, input_file: str):\n",
    "    with open(input_file, 'r') as f:\n",
    "        fold = json.load(f)\n",
    "    print(fold.keys())\n",
    "    training_images = fold['training'] # Should be list\n",
    "    validation_images = fold['validation']\n",
    "    training = {i: image for i, image in enumerate(training_images)}\n",
    "    validation = {i: image for i, image in enumerate(validation_images)}\n",
    "    return {'training': training,\n",
    "            'validation': validation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a167f79-0130-45cf-be27-9a7620756812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args={\n",
    "    't1_path':'/red/ruogu.fang/UKB/data/Brain/20252_T1_NIFTI/T1_unzip',\n",
    "    't2_path':'/red/ruogu.fang/UKB/data/Brain/20253_T2_NIFTI/T2_unzip'\n",
    "    }\n",
    "input_file=\"GBR_T1T2_matched.json\"\n",
    "data = load_T1T2matched_datalist(args, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06b331c2-4dc9-4e3a-b811-a3ab85fca132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39504\n",
      "1896\n"
     ]
    }
   ],
   "source": [
    "print(len(data[\"training\"]))\n",
    "print(len(data[\"validation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bec2e0c7-0261-484b-85f6-9812c483ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_T1T2mixed_datalist(args, input_file: str):\n",
    "    with open(input_file, 'r') as f:\n",
    "        fold = json.load(f)\n",
    "    print(fold.keys())\n",
    "    training_images = fold['training'] # Should be list\n",
    "    validation_images = fold['validation']\n",
    "    #training = {}\n",
    "    #for i, image in enumerate(training_images):\n",
    "    #    training[i] = image # here is pair\n",
    "    #training[i] = image # here is pair\n",
    "    training = {i: image for i, image in enumerate(training_images)}\n",
    "    #validation = {}\n",
    "    #for i, image in enumerate(validation_images):\n",
    "    #    validation[i] = image \n",
    "    validation = {i: image for i, image in enumerate(validation_images)}\n",
    "    return {'training': training,\n",
    "            'validation': validation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "136f97f1-2f1b-49ad-9493-c11a7cd182bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['training', 'validation'])\n"
     ]
    }
   ],
   "source": [
    "args={\n",
    "    't1_path':'/red/ruogu.fang/UKB/data/Brain/20252_T1_NIFTI/T1_unzip',\n",
    "    't2_path':'/red/ruogu.fang/UKB/data/Brain/20253_T2_NIFTI/T2_unzip'\n",
    "    }\n",
    "input_file=\"GBR_T1_T2_matched.json\"\n",
    "data = load_T1T2mixed_datalist(args, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "89168a52-183d-492f-927c-8ac1574e4a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82734"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['training'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1391ed6f-0f35-4549-b8cf-6f332aeb8dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b87ba-1319-4d84-8e9c-19d26406c9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe148d-3701-4fb4-afdd-cc54da895da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch timm",
   "language": "python",
   "name": "torch_timm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
