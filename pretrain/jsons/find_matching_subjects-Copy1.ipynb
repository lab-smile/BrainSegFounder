{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f79f6419-a262-4d73-8b07-d100a61db72f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7be63af4-28bb-4a1a-9a6e-6e7146fbad5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## CONSTANTS\n",
    "DATA_DIR = '/red/ruogu.fang/share/UKB/data/Brain'\n",
    "T1_FILE_PATH = '20252_T1_NIFTI'\n",
    "T2_FILE_PATH = '20253_T2_NIFTI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "744496af-1727-414c-b9b0-0178a89fc388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = Path(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7202dc8-f381-4505-afaf-be5b41b97d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_id_from_file(filename: str) -> str: \n",
    "    '''Split patient ID from a filename. Note that this function\n",
    "    does not take into account the visit number, nor number of s\n",
    "    '''\n",
    "    return filename.split('_')[0]  # First section is id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac13908b-e064-4b21-98db-cf78f311c6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ids_to_json(ids: set, outdir: os.PathLike, \n",
    "                name='GatorBrain_matched_subjects.json') -> None:\n",
    "    outdir = Path(outdir)\n",
    "    with open(outdir / name, 'w') as json_file:\n",
    "        json.dump(list(ids), json_file)\n",
    "    print(f'Saved matching subject IDs to {outdir / name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45e46d92-f54f-48ff-9992-0b5d2212c1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1_path = data_path / T1_FILE_PATH\n",
    "t2_path = data_path / T2_FILE_PATH / 'T2_unzip'\n",
    "\n",
    "# T1 data is stored in two different areas\n",
    "new_t1_path = t1_path / 'T1_new_unzip'\n",
    "old_t1_path = t1_path / 'T1_unzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e57c9964-9b9c-4a9d-a910-989438d5e42c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_t1_ids = [split_id_from_file(filename.name) \n",
    "              for filename in old_t1_path.iterdir()]\n",
    "\n",
    "new_t1_ids = [split_id_from_file(filename.name) \n",
    "              for filename in new_t1_path.iterdir()]\n",
    "\n",
    "t1_ids = set(old_t1_ids).union(set(new_t1_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ca45828-870e-4656-a7ea-3342d348b594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t2_ids = [split_id_from_file(filename.name)\n",
    "          for filename in t2_path.iterdir()]\n",
    "t2_ids = set(t2_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d64a61d3-6419-4183-a932-4a4e2c034af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_ids = t1_ids.intersection(t2_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28d4436e-dce1-4c12-89bb-5df91f94d26f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 44172 unique T1 subjects\n",
      "Total of 43369 unique T2 subjects\n",
      "Total of 43367 matching subjects found\n",
      "Saved matching subject IDs to GatorBrain_matched_subjects.json\n"
     ]
    }
   ],
   "source": [
    "print(f'Total of {len(t1_ids)} unique T1 subjects')\n",
    "print(f'Total of {len(t2_ids)} unique T2 subjects')\n",
    "print(f'Total of {len(matching_ids)} matching subjects found')\n",
    "ids_to_json(matching_ids, outdir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3ff2fa3-97bf-4844-b823-205a72094edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datalist = list(matching_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ba7f60a-7d01-4cb8-86c2-043b22782c7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/red/ruogu.fang/share/UKB/data/Brain/20252_T1_NIFTI/T1_unzip\n",
      "/red/ruogu.fang/share/UKB/data/Brain/20252_T1_NIFTI/T1_new_unzip\n"
     ]
    }
   ],
   "source": [
    "print(old_t1_path)\n",
    "print(new_t1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509e532-d9e8-4b7a-b9b9-333da43ecaf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Format 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f50f980-1893-4073-811f-c944affad4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unused T1 images = 1293\n",
      "number of unused T2 images = 1848\n",
      "number of usable subjects with both T1 + T2 images = 39491\n"
     ]
    }
   ],
   "source": [
    "train_datalist=[]\n",
    "unused_T1 = []\n",
    "unused_T2 = []\n",
    "for subject in datalist[:-2000]:\n",
    "    old_image_t1 = str(old_t1_path) + '/' + subject + \"_20252_2_0/T1_brain_to_MNI.nii.gz\"\n",
    "    new_image_t1 = str(new_t1_path) + '/' + subject + \"_20252_2_0/T1/T1_brain_to_MNI.nii.gz\"\n",
    "    image_t2 = str(t2_path) + '/' + subject + \"_20253_2_0/T2_FLAIR/T2_FLAIR_brain_to_MNI.nii.gz\"\n",
    "\n",
    "    usable=True\n",
    "    \n",
    "    if os.path.exists(old_image_t1):\n",
    "        image_t1 = old_image_t1 \n",
    "    elif os.path.exists(new_image_t1):\n",
    "        image_t1 = new_image_t1 \n",
    "    else:\n",
    "        #print(f\"subject:{subject} T1 not found\")\n",
    "        usable=False\n",
    "        unused_T1.append(subject)\n",
    "        \n",
    "    if os.path.exists(image_t2):\n",
    "        pass\n",
    "    else:\n",
    "        #print(f\"subject:{subject} T2 not found\")\n",
    "        usable=False\n",
    "        unused_T2.append(subject)\n",
    "    \n",
    "    if usable:\n",
    "        train_datalist.append({\"image_T1\": image_t1, \"image_T2\": image_t2})\n",
    "\n",
    "print(f\"number of unused T1 images = {len(unused_T1)}\")\n",
    "print(f\"number of unused T2 images = {len(unused_T2)}\")\n",
    "print(f\"number of usable subjects with both T1 + T2 images = {len(train_datalist)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c18bac0-3ff2-4442-8568-f76ea9992ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unused_T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0de722a-9559-4120-b9fe-c4e0fd7f0e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unused_T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e0c0190-7257-41aa-9e6e-f4c726c54f49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unused T1 images = 63\n",
      "number of unused T2 images = 90\n",
      "number of usable subjects with both T1 + T2 images = 1909\n"
     ]
    }
   ],
   "source": [
    "val_datalist=[]\n",
    "unused_T1 = []\n",
    "unused_T2 = []\n",
    "for subject in datalist[-2000:]:\n",
    "    old_image_t1 = str(old_t1_path) + '/' + subject + \"_20252_2_0/T1_brain_to_MNI.nii.gz\"\n",
    "    new_image_t1 = str(new_t1_path) + '/' + subject + \"_20252_2_0/T1/T1_brain_to_MNI.nii.gz\"\n",
    "    image_t2 = str(t2_path) + '/' + subject + \"_20253_2_0/T2_FLAIR/T2_FLAIR_brain_to_MNI.nii.gz\"\n",
    "\n",
    "    usable=True\n",
    "    \n",
    "    if os.path.exists(old_image_t1):\n",
    "        image_t1 = old_image_t1 \n",
    "    elif os.path.exists(new_image_t1):\n",
    "        image_t1 = new_image_t1 \n",
    "    else:\n",
    "        #print(f\"subject:{subject} T1 not found\")\n",
    "        usable=False\n",
    "        unused_T1.append(subject)\n",
    "        \n",
    "    if os.path.exists(image_t2):\n",
    "        pass\n",
    "    else:\n",
    "        #print(f\"subject:{subject} T2 not found\")\n",
    "        usable=False\n",
    "        unused_T2.append(subject)\n",
    "    \n",
    "    if usable:\n",
    "        val_datalist.append({\"image_T1\": image_t1, \"image_T2\": image_t2})\n",
    "\n",
    "print(f\"number of unused T1 images = {len(unused_T1)}\")\n",
    "print(f\"number of unused T2 images = {len(unused_T2)}\")\n",
    "print(f\"number of usable subjects with both T1 + T2 images = {len(val_datalist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4bcbd34c-80fd-429e-a41e-1330869d700e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\"training\":train_datalist,\n",
    "        \"validation\":val_datalist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "047fa6b6-3a6b-4bd1-bd40-00ad547faea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"GBR_T1T2_matched.json\", 'w') as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f57ae7b-9e3a-42b2-8ef4-8139e6c29d72",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Format 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98eb6a88-e575-4167-b42b-5f67f4b22078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_datalist=[]\n",
    "unused_T1 = []\n",
    "unused_T2 = []\n",
    "for subject in datalist[:-2000]:\n",
    "    old_image_t1 = str(old_t1_path) + '/' + subject + \"_20252_2_0/T1_brain_to_MNI.nii.gz\"\n",
    "    new_image_t1 = str(new_t1_path) + '/' + subject + \"_20252_2_0/T1/T1_brain_to_MNI.nii.gz\"\n",
    "    image_t2 = str(t2_path) + '/' + subject + \"_20253_2_0/T2_FLAIR/T2_FLAIR_brain_to_MNI.nii.gz\"\n",
    "\n",
    "    usable=True\n",
    "    \n",
    "    if os.path.exists(old_image_t1):\n",
    "        image_t1 = old_image_t1 \n",
    "    elif os.path.exists(new_image_t1):\n",
    "        image_t1 = new_image_t1 \n",
    "    else:\n",
    "        #print(f\"subject:{subject} T1 not found\")\n",
    "        usable=False\n",
    "        unused_T1.append(subject)\n",
    "        \n",
    "    if os.path.exists(image_t2):\n",
    "        pass\n",
    "    else:\n",
    "        #print(f\"subject:{subject} T2 not found\")\n",
    "        usable=False\n",
    "        unused_T2.append(subject)\n",
    "    \n",
    "    if usable:\n",
    "        train_datalist.append({\"image\": [image_t1, image_t2]})\n",
    "\n",
    "print(f\"Training\")\n",
    "print(f\"number of unused T1 images = {len(unused_T1)}\")\n",
    "print(f\"number of unused T2 images = {len(unused_T2)}\")\n",
    "print(f\"number of usable subjects with both T1 + T2 images = {len(train_datalist)}\")\n",
    "\n",
    "\n",
    "val_datalist=[]\n",
    "unused_T1 = []\n",
    "unused_T2 = []\n",
    "for subject in datalist[-2000:]:\n",
    "    old_image_t1 = str(old_t1_path) + '/' + subject + \"_20252_2_0/T1_brain_to_MNI.nii.gz\"\n",
    "    new_image_t1 = str(new_t1_path) + '/' + subject + \"_20252_2_0/T1/T1_brain_to_MNI.nii.gz\"\n",
    "    image_t2 = str(t2_path) + '/' + subject + \"_20253_2_0/T2_FLAIR/T2_FLAIR_brain_to_MNI.nii.gz\"\n",
    "\n",
    "    usable=True\n",
    "    \n",
    "    if os.path.exists(old_image_t1):\n",
    "        image_t1 = old_image_t1 \n",
    "    elif os.path.exists(new_image_t1):\n",
    "        image_t1 = new_image_t1 \n",
    "    else:\n",
    "        #print(f\"subject:{subject} T1 not found\")\n",
    "        usable=False\n",
    "        unused_T1.append(subject)\n",
    "        \n",
    "    if os.path.exists(image_t2):\n",
    "        pass\n",
    "    else:\n",
    "        #print(f\"subject:{subject} T2 not found\")\n",
    "        usable=False\n",
    "        unused_T2.append(subject)\n",
    "    \n",
    "    if usable:\n",
    "        val_datalist.append({\"image\": [image_t1, image_t2]})\n",
    "\n",
    "print(f\"Validation\")\n",
    "print(f\"number of unused T1 images = {len(unused_T1)}\")\n",
    "print(f\"number of unused T2 images = {len(unused_T2)}\")\n",
    "print(f\"number of usable subjects with both T1 + T2 images = {len(val_datalist)}\")\n",
    "\n",
    "data = {\"training\":train_datalist,\n",
    "        \"validation\":val_datalist}\n",
    "\n",
    "with open(\"GBR_T1T2_matched_image.json\", 'w') as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83f96a4-b552-4b54-80c0-522c24e6b35d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_T1T2matched_datalist(args, input_file: str):\n",
    "    with open(input_file, 'r') as f:\n",
    "        fold = json.load(f)\n",
    "    print(fold.keys())\n",
    "    training_images = fold['training'] # Should be list\n",
    "    validation_images = fold['validation']\n",
    "    training = {i: image for i, image in enumerate(training_images)}\n",
    "    validation = {i: image for i, image in enumerate(validation_images)}\n",
    "    return {'training': training,\n",
    "            'validation': validation}\n",
    "\n",
    "def get_T1T2_dataloaders(args, modality=\"T1\", num_workers = 4):\n",
    "  \n",
    "    datalist = load_T1T2matched_datalist(args, args[\"splits\"])\n",
    "\n",
    "    if modality == \"T1\":\n",
    "        training_datalist   = [{\"image\":subject[0]} for subject in datalist['training']] \n",
    "        validation_datalist = [{\"image\":subject[0]} for subject in datalist['validation']] \n",
    "    elif modality == \"T2\":\n",
    "        training_datalist   = [{\"image\":subject[1]} for subject in datalist['training']] \n",
    "        validation_datalist = [{\"image\":subject[1]} for subject in datalist['validation']] \n",
    "    elif modality == \"T1_T2\":\n",
    "        training_datalist, validation_datalist = datalist['training'], datalist['validation']\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported modality\")\n",
    "    return training_datalist, validation_datalist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2a2054-0f31-4dbf-9d85-533a2876bc19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args={\n",
    "'splits':'GBR_T1T2_matched_image.json'\n",
    "}\n",
    "data = load_T1T2matched_datalist(args,'GBR_T1T2_matched_image.json') \n",
    "print(data[\"training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb6cfd-8da7-4e0c-8f26-d8f3dbc2a786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "440cd5c3-904b-45bd-92e5-e442fd15bfe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Format Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ce74704-8e10-4f3d-8767-eea71be2c484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_T1T2mixed_datalist(args, input_file: str):\n",
    "    with open(input_file, 'r') as f:\n",
    "        fold = json.load(f)\n",
    "    print(fold.keys())\n",
    "    training_images = fold['training'] # Should be list\n",
    "    validation_images = fold['validation']\n",
    "    t1_path = args['t1_path'] #'/red/ruogu.fang/UKB/data/Brain/20252_T1_NIFTI/T1_unzip'\n",
    "    t2_path = args['t2_path'] #'/red/ruogu.fang/UKB/data/Brain/20253_T2_NIFTI/T2_unzip'\n",
    "    training = {}\n",
    "    for i, image in enumerate(training_images):\n",
    "        image_t1 = t1_path + '/' + image + \"_20252_2_0/T1_brain_to_MNI.nii.gz\"\n",
    "        image_t2 = t2_path + '/' + image + \"_20253_2_0/T2_FLAIR/T2_FLAIR_brain_to_MNI.nii.gz\"\n",
    "        training[i] = {\"T1_image\": image_t1,\n",
    "                       \"T2_image\": image_t2}\n",
    "\n",
    "    validation = {}\n",
    "    for i, image in enumerate(validation_images):\n",
    "        image_t1 = t1_path + '/' + image + \"_20252_2_0/T1_brain_to_MNI.nii.gz\"\n",
    "        image_t2 = t2_path + '/' + image + \"_20253_2_0/T2_FLAIR/T2_FLAIR_brain_to_MNI.nii.gz\"\n",
    "        validation[i] = {\"T1_image\": image_t1,\n",
    "                         \"T2_image\": image_t2}\n",
    "    return {'training': training,\n",
    "            'validation': validation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02f5d808-584b-409e-8206-98a545b9d3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['training', 'validation'])\n"
     ]
    }
   ],
   "source": [
    "args={\n",
    "    't1_path':'/red/ruogu.fang/UKB/data/Brain/20252_T1_NIFTI/T1_unzip',\n",
    "    't2_path':'/red/ruogu.fang/UKB/data/Brain/20253_T2_NIFTI/T2_unzip'\n",
    "    }\n",
    "input_file=\"GBR_T1_T2_matched.json\"\n",
    "data = load_T1T2mixed_datalist(args, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ad5494c-6a9a-49c8-bc7e-07bb78761880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e20688-bde0-402a-96b9-3511c15f808a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a5496-2098-4d85-8605-7cb272aec540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dce253e4-ca03-4117-8c6c-0ddb096d7d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_T1T2matched_datalist(args, input_file: str):\n",
    "    with open(input_file, 'r') as f:\n",
    "        fold = json.load(f)\n",
    "    print(fold.keys())\n",
    "    training_images = fold['training'] # Should be list\n",
    "    validation_images = fold['validation']\n",
    "    training = {i: image for i, image in enumerate(training_images)}\n",
    "    validation = {i: image for i, image in enumerate(validation_images)}\n",
    "    return {'training': training,\n",
    "            'validation': validation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a167f79-0130-45cf-be27-9a7620756812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args={\n",
    "    't1_path':'/red/ruogu.fang/UKB/data/Brain/20252_T1_NIFTI/T1_unzip',\n",
    "    't2_path':'/red/ruogu.fang/UKB/data/Brain/20253_T2_NIFTI/T2_unzip'\n",
    "    }\n",
    "input_file=\"GBR_T1T2_matched.json\"\n",
    "data = load_T1T2matched_datalist(args, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06b331c2-4dc9-4e3a-b811-a3ab85fca132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39504\n",
      "1896\n"
     ]
    }
   ],
   "source": [
    "print(len(data[\"training\"]))\n",
    "print(len(data[\"validation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bec2e0c7-0261-484b-85f6-9812c483ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_T1T2mixed_datalist(args, input_file: str):\n",
    "    with open(input_file, 'r') as f:\n",
    "        fold = json.load(f)\n",
    "    print(fold.keys())\n",
    "    training_images = fold['training'] # Should be list\n",
    "    validation_images = fold['validation']\n",
    "    #training = {}\n",
    "    #for i, image in enumerate(training_images):\n",
    "    #    training[i] = image # here is pair\n",
    "    #training[i] = image # here is pair\n",
    "    training = {i: image for i, image in enumerate(training_images)}\n",
    "    #validation = {}\n",
    "    #for i, image in enumerate(validation_images):\n",
    "    #    validation[i] = image \n",
    "    validation = {i: image for i, image in enumerate(validation_images)}\n",
    "    return {'training': training,\n",
    "            'validation': validation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "136f97f1-2f1b-49ad-9493-c11a7cd182bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['training', 'validation'])\n"
     ]
    }
   ],
   "source": [
    "args={\n",
    "    't1_path':'/red/ruogu.fang/UKB/data/Brain/20252_T1_NIFTI/T1_unzip',\n",
    "    't2_path':'/red/ruogu.fang/UKB/data/Brain/20253_T2_NIFTI/T2_unzip'\n",
    "    }\n",
    "input_file=\"GBR_T1_T2_matched.json\"\n",
    "data = load_T1T2mixed_datalist(args, input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "89168a52-183d-492f-927c-8ac1574e4a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82734"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['training'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1391ed6f-0f35-4549-b8cf-6f332aeb8dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b87ba-1319-4d84-8e9c-19d26406c9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe148d-3701-4fb4-afdd-cc54da895da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch timm",
   "language": "python",
   "name": "torch_timm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
